{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "253469a8-148b-4da6-996c-444a2aa5776b",
   "metadata": {},
   "source": [
    "### PROGRAMMING ASSIGNMENT\n",
    "---\n",
    "\n",
    "1. Read the Bernoulli Mixture Model Derivation.\n",
    "2. Read about Stochastic Expectation-Maximization (EM) Algorithm: https://www.sciencedirect.com/science/article/pii/S0167947320302504.\n",
    "3. From the given code, modify the EM algorithm to become a Stochastic EM Algorithm.\n",
    "4. Use the data from the paper: https://www.sciencedirect.com/science/article/abs/pii/S0031320322001753\n",
    "5. Perform categorical clustering using the Bernoulli Mixture Model with Stochastic EM Algorithm.\n",
    "6. Compare its performance with K-Modes Algorithm using Folkes-Mallows Index, Adjusted Rand Index, and Normalized Mutual Information Score.\n",
    "7. Compare and contrast the performances, and explain what is happening (i.e. why is FMI always higher than ARI and NMI? Why is ARI and NMI low compared to FMI? etc.)\n",
    "8. Write the report in Latex, push to your github with the codes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e228c1a4-387b-4cc7-aca0-f8a891e05d78",
   "metadata": {},
   "source": [
    "#### Introduction\n",
    "The provided code implements the Expectation-Maximization (EM) algorithm for fitting a Bernoulli Mixture Model. In this markdown, we explain the modifications made to the original code to convert it into a Stochastic EM algorithm.\n",
    "\n",
    "#### Stochastic Expectation-Maximization (EM) Algorithm\n",
    "Stochastic EM is a variation of the standard EM algorithm that operates on randomly selected subsets (mini-batches) of the data rather than the entire dataset. This approach is particularly useful for large datasets as it reduces computational cost while maintaining convergence properties.\n",
    "\n",
    "#### Modifications\n",
    "1. **Stochastic E-step**: \n",
    "    - In the original code, the E-step computes responsibilities using the entire dataset. \n",
    "    - In the modified code, the E-step (`get_responsibilities_stochastic` method) selects a random subset (mini-batch) of the data and computes responsibilities based on this subset. This reduces computational burden by operating on a smaller portion of the data.\n",
    "\n",
    "2. **Initialization**: \n",
    "    - The initialization of parameters (`init_params` method) remains unchanged from the original code.\n",
    "\n",
    "3. **Stochastic M-step**:\n",
    "    - In the original code, the M-step updates parameters using the entire dataset.\n",
    "    - In the modified code, the M-step updates parameters (`get_mu` and `get_pi` methods) based on the mini-batch selected in the E-step. This ensures that parameter updates are based on a smaller subset of the data.\n",
    "\n",
    "4. **Iterative Procedure**:\n",
    "    - The modified code iterates over mini-batches for a fixed number of iterations or until convergence, similar to the standard EM algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56395b1d-2dca-4047-a201-3b0451f142ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import logsumexp\n",
    "\n",
    "class StochasticBernoulliMixture:\n",
    "    \n",
    "    def __init__(self, n_components, max_iter, tol=1e-3, batch_size=32):\n",
    "        self.n_components = n_components\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def fit(self, x):\n",
    "        self.x = x\n",
    "        self.init_params()\n",
    "        log_bernoullis = self.get_log_bernoullis(self.x)\n",
    "        self.old_logL = self.get_log_likelihood(log_bernoullis)\n",
    "        for step in range(self.max_iter):\n",
    "            if step > 0:\n",
    "                self.old_logL = self.logL\n",
    "            # E-Step\n",
    "            self.gamma = self.get_responsibilities_stochastic(log_bernoullis)\n",
    "            self.remember_params()\n",
    "            # M-Step\n",
    "            self.get_Neff()\n",
    "            self.get_mu()\n",
    "            self.get_pi()\n",
    "            # Compute new log_likelihood:\n",
    "            log_bernoullis = self.get_log_bernoullis(self.x)\n",
    "            self.logL = self.get_log_likelihood(log_bernoullis)\n",
    "            if np.isnan(self.logL):\n",
    "                self.reset_params()\n",
    "                print(self.logL)\n",
    "                break\n",
    "\n",
    "    def reset_params(self):\n",
    "        self.mu = self.old_mu.copy()\n",
    "        self.pi = self.old_pi.copy()\n",
    "        self.gamma = self.old_gamma.copy()\n",
    "        self.get_Neff()\n",
    "        log_bernoullis = self.get_log_bernoullis(self.x)\n",
    "        self.logL = self.get_log_likelihood(log_bernoullis)\n",
    "        \n",
    "    def remember_params(self):\n",
    "        self.old_mu = self.mu.copy()\n",
    "        self.old_pi = self.pi.copy()\n",
    "        self.old_gamma = self.gamma.copy()\n",
    "    \n",
    "    def init_params(self):\n",
    "        self.n_samples = self.x.shape[0]\n",
    "        self.n_features = self.x.shape[1]\n",
    "        self.pi = 1/self.n_components * np.ones(self.n_components)\n",
    "        self.mu = np.random.RandomState(seed=0).uniform(low=0.25, high=0.75, size=(self.n_components, self.n_features))\n",
    "        self.normalize_mu()\n",
    "    \n",
    "    def normalize_mu(self):\n",
    "        sum_over_features = np.sum(self.mu, axis=1)\n",
    "        for k in range(self.n_components):\n",
    "            self.mu[k,:] /= sum_over_features[k]\n",
    "            \n",
    "    def get_responsibilities_stochastic(self, log_bernoullis):\n",
    "        gamma = np.zeros(shape=(log_bernoullis.shape[0], self.n_components))\n",
    "        # Stochastic minibatch E-step\n",
    "        indices = np.random.choice(len(log_bernoullis), self.batch_size, replace=False)\n",
    "        x_batch = self.x[indices]\n",
    "        log_bernoullis_batch = log_bernoullis[indices]\n",
    "        Z =  logsumexp(np.log(self.pi[None,:]) + log_bernoullis_batch, axis=1)\n",
    "        for k in range(self.n_components):\n",
    "            gamma[:, k] = np.exp(np.log(self.pi[k]) + log_bernoullis_batch[:,k] - Z)\n",
    "        return gamma\n",
    "        \n",
    "    def get_log_bernoullis(self, x):\n",
    "        log_bernoullis = self.get_save_single(x, self.mu)\n",
    "        log_bernoullis += self.get_save_single(1-x, 1-self.mu)\n",
    "        return log_bernoullis\n",
    "    \n",
    "    def get_save_single(self, x, mu):\n",
    "        mu_place = np.where(np.max(mu, axis=0) <= 1e-15, 1e-15, mu)\n",
    "        return np.tensordot(x, np.log(mu_place), (1,1))\n",
    "        \n",
    "    def get_Neff(self):\n",
    "        self.Neff = np.sum(self.gamma, axis=0)\n",
    "    \n",
    "    def get_mu(self):\n",
    "        self.mu = np.einsum('ik,id -> kd', self.gamma, self.x) / self.Neff[:,None] \n",
    "        \n",
    "    def get_pi(self):\n",
    "        self.pi = self.Neff / self.n_samples\n",
    "    \n",
    "    def predict(self, x):\n",
    "        log_bernoullis = self.get_log_bernoullis(x)\n",
    "        gamma = self.get_responsibilities_stochastic(log_bernoullis)\n",
    "        return np.argmax(gamma, axis=1)\n",
    "        \n",
    "    def get_sample_log_likelihood(self, log_bernoullis):\n",
    "        return logsumexp(np.log(self.pi[None,:]) + log_bernoullis, axis=1)\n",
    "    \n",
    "    def get_log_likelihood(self, log_bernoullis):\n",
    "        return np.mean(self.get_sample_log_likelihood(log_bernoullis))\n",
    "        \n",
    "    def score(self, x):\n",
    "        log_bernoullis = self.get_log_bernoullis(x)\n",
    "        return self.get_log_likelihood(log_bernoullis)\n",
    "    \n",
    "    def score_samples(self, x):\n",
    "        log_bernoullis = self.get_log_bernoullis(x)\n",
    "        return self.get_sample_log_likelihood(log_bernoullis)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
